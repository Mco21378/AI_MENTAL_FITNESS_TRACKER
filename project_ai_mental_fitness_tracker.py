# -*- coding: utf-8 -*-
"""PROJECT-AI MENTAL FITNESS TRACKER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x3lm7Bg43MSINBayYb2uBh6ayuPfNPQs
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

from google.colab import drive
drive.mount('/content/drive')

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score

# Load the datasets
df1 = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/prevalence-by-mental-and-substance-use-disorder.csv")
df2 = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/mental-and-substance-use-as-share-of-disease (1).csv")

# dataset 1
df1.head()

# dataset 2
df2.head()

# Merge the datasets
data = pd.merge(df1, df2)
data.head()

# Drop unnecessary columns
data.drop('Code', axis=1, inplace=True)

data.size,data.shape

# Rename columns
data.columns = ['Country', 'Year', 'Schizophrenia', 'Bipolar_disorder', 'Eating_disorder', 'Anxiety', 'Drug_usage',
               'Depression', 'Alcohol', 'Mental_fitness']

data.head()

"""## **EXPLORATORY ANALYSIS**"""

# Visualize the correlations
plt.figure(figsize=(12, 6))
sns.heatmap(data.corr(), annot=True, cmap='Greens')
plt.plot()

sns. jointplot( x ='Schizophrenia', y ='Mental_fitness',data = data, color='m')
plt.show()

sns.jointplot(x='Bipolar_disorder', y='Mental_fitness', data=data,  cmap='blue')
plt.show()

# Visualize pairwise relationships
sns.pairplot(data, corner=True)
plt.show()

fig=px.bar(data.head(10),x='Year',y='Mental_fitness',color='Year',template='ggplot2')
fig.show()

mean = data['Mental_fitness'].mean()
mean

# Encode non-numeric labels
label_encoder = LabelEncoder()
for column in data.columns:
    if data[column].dtype == 'object':
        data[column] = label_encoder.fit_transform(data[column])

# Split the data into training and testing sets
X = data.drop('Mental_fitness', axis=1)
y = data['Mental_fitness']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

""" **LINEAR REGRESSION**"""

# Train the Linear Regression model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Model evaluation for training set
y_train_pred = lr.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_train_pred)
print("Linear Regression model performance for training set:")
print("MSE: ", mse_train)
print("RMSE: ", rmse_train)
print("R2 score: ", r2_train)

""" **RANDOM FOREST REGRESSOR**"""

# Train the Random Forest Regressor model
rf = RandomForestRegressor()
rf.fit(X_train, y_train)

# Model evaluation for training set
y_train_pred = rf.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_train_pred)
print("\nRandom Forest Regressor model performance for training set:")
print("MSE: ", mse_train)
print("RMSE: ", rmse_train)
print("R2 score: ", r2_train)

# Feature importance analysis
importance = rf.feature_importances_
feature_names = X.columns
sorted_indices = np.argsort(importance)[::-1]
sorted_importance = importance[sorted_indices]
sorted_features = feature_names[sorted_indices]
plt.figure(figsize=(10, 6))
sns.barplot(x=sorted_importance, y=sorted_features)
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Random Forest Regressor - Feature Importance")
plt.show()

# Model evaluation for testing set (Linear Regression)
y_test_pred = lr.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_test_pred)
print("\nLinear Regression model performance for testing set:")
print("MSE: ", mse_test)
print("RMSE: ", rmse_test)
print("R2 score: ", r2_test)

# Model evaluation for testing set (Random Forest Regressor)
y_test_pred = rf.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_test_pred)
print("\nRandom Forest Regressor model performance for testing set:")
print("MSE: ", mse_test)
print("RMSE: ", rmse_test)
print("R2 score: ", r2_test)

# Perform cross-validation for Linear Regression
lr_cv_scores = cross_val_score(lr, X, y, cv=5, scoring='r2')
print("Cross-validation scores for Linear Regression:")
print(lr_cv_scores)
print("Average R2 score: ", np.mean(lr_cv_scores))

# Perform cross-validation for Random Forest Regressor
rf_cv_scores = cross_val_score(rf, X, y, cv=5, scoring='r2')
print("\nCross-validation scores for Random Forest Regressor:")
print(rf_cv_scores)
print("Average R2 score: ", np.mean(rf_cv_scores))

# Feature importance for Random Forest Regressor
feature_importance = rf.feature_importances_
feature_names = X.columns

# Sort feature importance in descending order
sorted_indices = np.argsort(feature_importance)[::-1]
sorted_feature_names = feature_names[sorted_indices]
sorted_feature_importance = feature_importance[sorted_indices]

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.bar(range(len(sorted_feature_importance)), sorted_feature_importance, tick_label=sorted_feature_names)
plt.xticks(rotation=90)
plt.xlabel("Features")
plt.ylabel("Importance")
plt.title("Feature Importance")
plt.show()

# Predictions and residuals
y_test_pred_lr = lr.predict(X_test)
residuals_lr = y_test - y_test_pred_lr

y_test_pred_rf = rf.predict(X_test)
residuals_rf = y_test - y_test_pred_rf

# Scatter plot of predicted vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred_lr, label="Linear Regression", alpha=0.7)
plt.scatter(y_test, y_test_pred_rf, label="Random Forest Regressor", alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
plt.legend()
plt.show()

# Residual plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test_pred_lr, residuals_lr, label="Linear Regression", alpha=0.7)
plt.scatter(y_test_pred_rf, residuals_rf, label="Random Forest Regressor", alpha=0.7)
plt.axhline(y=0, color='k', linestyle='--', linewidth=2)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.legend()
plt.show()

"""**Hyperparameter Tuning for Linear Regression**:

Although Linear Regression doesn't have many hyperparameters to tune, you can still use techniques like cross-validation to find the best hyperparameter values. You can create a parameter grid for the hyperparameters and use GridSearchCV to perform the search.
python

"""

from sklearn.model_selection import GridSearchCV

# Create a new instance of Linear Regression
lr = LinearRegression()

# Hyperparameter tuning for Linear Regression
param_grid_lr = {
    'fit_intercept': [True, False],
    'copy_X': [True, False]
}

grid_search_lr = GridSearchCV(estimator=lr, param_grid=param_grid_lr, cv=5, scoring='r2', n_jobs=-1)
grid_search_lr.fit(X_train, y_train)

best_params_lr = grid_search_lr.best_params_
best_lr = grid_search_lr.best_estimator_

# Model evaluation for testing set (Linear Regression with best hyperparameters)
y_test_pred_lr_best = best_lr. predict(X_test)
mse_test_lr_best = mean_squared_error(y_test, y_test_pred_lr_best)
rmse_test_lr_best = np.sqrt(mse_test_lr_best)
r2_test_lr_best = r2_score(y_test, y_test_pred_lr_best)

print("\nLinear Regression (Best Hyperparameters) model performance for testing set:")
print("Best Hyperparameters: ", best_params_lr)
print("MSE: ", mse_test_lr_best)
print("RMSE: ", rmse_test_lr_best)
print("R2 score: ", r2_test_lr_best)

import joblib

# Save the best model
joblib.dump(best_lr, 'best_linear_regression_model.pkl')

# Load the saved model
loaded_model = joblib.load('best_linear_regression_model.pkl')

# Use the loaded model for predictions
y_test_pred_loaded = loaded_model.predict(X_test)
mse_test_loaded = mean_squared_error(y_test, y_test_pred_loaded)
rmse_test_loaded = np.sqrt(mse_test_loaded)
r2_test_loaded = r2_score(y_test, y_test_pred_loaded)
print("\nLoaded Linear Regression model performance for testing set:")
print("MSE: ", mse_test_loaded)
print("RMSE: ", rmse_test_loaded)
print("R2 score: ", r2_test_loaded)

"""**Model Comparison:**

You can compare the performance of multiple regression models using additional metrics such as Mean Absolute Error (MAE) and Explained Variance Score (EV). You can include other regression models like Ridge Regression, Lasso Regression, or Support Vector Regression (SVR) and evaluate their performance on the testing set.




"""

from sklearn.linear_model import Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, explained_variance_score

# Create instances of additional regression models
ridge = Ridge()
lasso = Lasso()
svr = SVR()

# Train the additional regression models
ridge.fit(X_train, y_train)
lasso.fit(X_train, y_train)
svr.fit(X_train, y_train)

# Model evaluation for testing set (Ridge Regression)
y_test_pred_ridge = ridge.predict(X_test)
mae_ridge = mean_absolute_error(y_test, y_test_pred_ridge)
ev_ridge = explained_variance_score(y_test, y_test_pred_ridge)

print("\nRidge Regression model performance for testing set:")
print("MAE: ", mae_ridge)
print("Explained Variance Score: ", ev_ridge)

# Model evaluation for testing set (Lasso Regression)
y_test_pred_lasso = lasso.predict(X_test)
mae_lasso = mean_absolute_error(y_test, y_test_pred_lasso)
ev_lasso = explained_variance_score(y_test, y_test_pred_lasso)

print("\nLasso Regression model performance for testing set:")
print("MAE: ", mae_lasso)
print("Explained Variance Score: ", ev_lasso)

# Model evaluation for testing set (SVR)
y_test_pred_svr = svr.predict(X_test)
mae_svr = mean_absolute_error(y_test, y_test_pred_svr)
ev_svr = explained_variance_score(y_test, y_test_pred_svr)

print("\nSVR model performance for testing set:")
print("MAE: ", mae_svr)
print("Explained Variance Score: ", ev_svr)

"""**Residual Analysis** :

You can analyze the residuals in more detail by plotting a histogram and a Q-Q plot. This helps you assess whether the residuals follow a normal distribution.
"""

import scipy.stats as stats

# Calculate residuals for Linear Regression
residuals_lr = y_test - y_test_pred_lr_best

# Plot histogram of residuals
plt.figure(figsize=(8, 6))
sns.histplot(residuals_lr, kde=True)
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.title("Histogram of Residuals (Linear Regression)")
plt.show()

# Plot Q-Q plot of residuals
plt.figure(figsize=(8, 6))
stats.probplot(residuals_lr, plot=plt)
plt.title("Q-Q Plot of Residuals (Linear Regression)")
plt.show()

"""**Feature Selection using Recursive Feature Elimination (RFE):**"""

from sklearn.feature_selection import RFE

# Create a new instance of Linear Regression
lr = LinearRegression()

# Perform feature selection with RFE
rfe = RFE(estimator=lr, n_features_to_select=5)  # Select the top 5 features
rfe.fit(X_train, y_train)

# Get the selected feature indices
selected_feature_indices = rfe.support_

# Filter the dataset based on selected features
X_train_selected = X_train.iloc[:, selected_feature_indices]
X_test_selected = X_test.iloc[:, selected_feature_indices]

"""**Cross-Validation using k-fold Cross-Validation:**"""

from sklearn.model_selection import cross_val_score

# Perform k-fold cross-validation
k = 5  # Number of folds
cv_scores = cross_val_score(lr, X_train_selected, y_train, cv=k, scoring='r2')

# Calculate the average cross-validation score
avg_cv_score = np.mean(cv_scores)
print("Average Cross-Validation Score:", avg_cv_score)